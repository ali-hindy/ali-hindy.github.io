<!DOCTYPE html>
<html lang="en" dir="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="noindex, nofollow">
<title>Ali Hindy</title>
<meta name="keywords" content="">
<meta name="description" content="Natural Language Processing What is Natural Language Processing (NLP)?
NLP is a field of computer science focused on dealing with words and human language, and building models to do operations on our language, from machine translation to text summarization to question answering. NLP has gained significant traction in the past three years with the advent of GPT-3.5 (ChatGPT), and will only get more and more relevant as time goes on and people build better and better models.">
<meta name="author" content="Ali Hindy">
<link rel="canonical" href="http://localhost:1313/blog/nlp-from-contextual-ai/natural-language-processing-99c197b09a1f451aa77fb5be55663fb7/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.a4c7743c7e78b08775cb013ed344962c383552b18400cdc6fcab7ac1cd9d3a52.css" integrity="sha256-pMd0PH54sId1ywE&#43;00SWLDg1UrGEAM3G/Kt6wc2dOlI=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/apple-touch-icon.png">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://localhost:1313/blog/nlp-from-contextual-ai/natural-language-processing-99c197b09a1f451aa77fb5be55663fb7/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript><meta property="og:title" content="" />
<meta property="og:description" content="Natural Language Processing What is Natural Language Processing (NLP)?
NLP is a field of computer science focused on dealing with words and human language, and building models to do operations on our language, from machine translation to text summarization to question answering. NLP has gained significant traction in the past three years with the advent of GPT-3.5 (ChatGPT), and will only get more and more relevant as time goes on and people build better and better models." />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://localhost:1313/blog/nlp-from-contextual-ai/natural-language-processing-99c197b09a1f451aa77fb5be55663fb7/" /><meta property="article:section" content="blog" />



<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content=""/>
<meta name="twitter:description" content="Natural Language Processing What is Natural Language Processing (NLP)?
NLP is a field of computer science focused on dealing with words and human language, and building models to do operations on our language, from machine translation to text summarization to question answering. NLP has gained significant traction in the past three years with the advent of GPT-3.5 (ChatGPT), and will only get more and more relevant as time goes on and people build better and better models."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Blog",
      "item": "http://localhost:1313/blog/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "",
      "item": "http://localhost:1313/blog/nlp-from-contextual-ai/natural-language-processing-99c197b09a1f451aa77fb5be55663fb7/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "",
  "name": "",
  "description": "Natural Language Processing What is Natural Language Processing (NLP)?\nNLP is a field of computer science focused on dealing with words and human language, and building models to do operations on our language, from machine translation to text summarization to question answering. NLP has gained significant traction in the past three years with the advent of GPT-3.5 (ChatGPT), and will only get more and more relevant as time goes on and people build better and better models.",
  "keywords": [
    
  ],
  "articleBody": "Natural Language Processing What is Natural Language Processing (NLP)?\nNLP is a field of computer science focused on dealing with words and human language, and building models to do operations on our language, from machine translation to text summarization to question answering. NLP has gained significant traction in the past three years with the advent of GPT-3.5 (ChatGPT), and will only get more and more relevant as time goes on and people build better and better models. At the core of NLP there exists two fundamental tasks - representation and generation, which allow us to build on to more complex ideas such as understanding or reasoning (specifically - sentiment analysis, question answering). Representation is the task of taking human tokens (such as natural English) and representing each token as a vector in high dimensional space. One desirata for these vectors is that they encode semantic meaning, and we can generate embeddings (vectors) for essentially anything, from words to sentences to entire passages. We have three phases of training our models - pretraining, finetuning, and inference. When a model is pre-trained, it means that it has been trained on a vast amount of text data using a technique called self-supervised learning. During pre-training, the model learns to predict missing words in a sentence, mask certain tokens, or predict the next token in a sequence, without needing explicit labels for each task. This process allows the model to develop a rich understanding of the language and its nuances. Finetuning adapts the pre-trained model to a specific dataset using supervised learning. Essentially take the pre-trained model and throw it more data in a specific use case to improve its contextualized performance. As one can imagine, one of the largest issues here is quality data curation, as your model is only as good as the data you give it. Now, let’s go over the two main model architectures in current NLP (as of Feb 2024) for processing sequential data - Recurrent Neural Networks (RNNs) and Transformers. The fundamental building blocks of these models are called encoders and decoders. An encoder takes some raw input data (say, raw text) and outputs a probability density function, specifying which word is most likely to come next in a sentence. The layer before this output is called the embedding layer which encodes our words into vectors with semantic meaning. A decoder takes those embedding layers as input and returns a sequence of tokens as output. A model with an encoder-decoder architecture is called a seq2seq (sequence to sequence) model, which takes in a sequence of tokens and returns a sequence of tokens. RNNs follow this type of architecture, with the most common architecture called a Long Short Term Memory (LSTM) model used for sentiment analysis or machine translation. LSTMs are designed to handle short-term and long-term dependencies in sequential data by maintaining a memory cell that can retain information over multiple time steps. However, they may struggle to capture very long-range dependencies due to issues like vanishing gradients. A seminal research paper called “Attention is All You Need” came out in 2017 and revolutionized the field of NLP. This paper introduced the notion of attention, which allows the model to focus on specific parts of the input sequence when making predictions or generating outputs, enabling it to capture long-range dependencies and improve performance on various NLP tasks. Attention operates using a key, value, query mechanism, which allows the model to compute attention scores, essentially telling the model what part of the sequence to pay more attention to. In the traditional encoder-decoder schema of a seq2seq model, our encoder and decoders are both RNNs that keep a hidden state after each time step $t$. At the last time step of the encoder model, we pass the hidden state $h_{t_n}$ as input to the decoder as context for the decoder. $h_{t_n}$ represents our representation of the input sequence. With attention, we pass as input to the decoder $h_{t_1},…,h_{t_n}$, all of the hidden layers from each time step of the encoder. Now, at every time step of the decoder, we assign a score to the hidden states, normalize these scores with softmax, and multiply each vector by its score to give more priority to higher score values. Then we take the sum of the weighted vectors to produce our output.\nExample of how attention works, as we have higher values at each time step of the decoder to reflect paying more attention to different parts of the sentence. More information about attention can be found here .\nThe notion of attention led to the transformer architecture. Transformers process the entire input sequence in parallel, allowing them to capture long-range dependencies more efficiently. Transformers use self-attention mechanisms to weigh the importance of different tokens in the input sequence, and most importantly do not have a fixed size memory cell like an LSTM. The most popular models nowadays (GPT, LLama, etc.) are some form of a transformer model. We can take a look a bit under the hood of how transformers actually work: In the figures above, we have $N=6$ encoders stacked on top of each other (note they do not share weights) and $N$ decoders stacked on top of each other. Each encoder consists of a self-attention (or multi-head attention, just self-attention done $x$ times), layer that feeds to a feed-forward neural network. The decoder is the same thing, except it receives attention values from the encoders, just like how we described attention before. Self-attention works very similarly to attention, except it is a way of looking through at other positions in our input sentence to provide more context for each word. For example, in the sentence “The animal didn't cross the street because it was too tired\", self-attention allows us to determine what “it” refers to in the context of the sentence.\nIn calculating self-attention, create a query, key, and value matrix for each word. Our score for each word w.r.t word$_i$ is the dot product of the query vector for word$_i$ and the key vector of the word we are scoring. Then softmax all of these values (to normalize between 0 and 1), multiply each value vector by these scores and we get our result.\nThis is what everything looks like when we add multi-head self-attention, and normalize after each step. The decoder looks the exact same, except it receives mutli-head attention from the last encoder (called $K$ and $V$).\nWait a second? How in the world do we go from the output of a decoder, which is a hidden state vector, to a word? We add one final linear layer and a softmax layer, which converts the hidden state vector to a much larger, sparse logits vector representing a one-hot encoding of our dictionary. After a softmax, we get a probability distribution over our dictionary, and we choose the word with the highest probability.\nOur label distribution might look like:\nWhereas our training output might look like:\nWhere at each time step, we are essentially comparing two probability distributions and minimizing the difference between the two on average throughout our entire dataset.\nTo put the transformer architecture in our framework of representation and generation, we can think of the stack of encoders as learning hierarchical, expressive representations (as opposed to just having one encoder). By having multiple encoders, we are able to learn different aspects of the input sequence’s representations to better understand complex information. Furthermore, we can think of the stack of decoders as generating a more complex, expressive sequence given our hierarchical, complex understanding of the input sequence from the encoders. Now let’s move to the next phase of an NLP pipeline - finetuning. In fine-tuning, the pre-trained model is typically initialized with weights learned from a large corpus or dataset using techniques like self-supervised learning. It is then fine-tuned on a smaller, task-specific dataset using supervised learning techniques. One interesting development that has taken off recently is Retrieval Augmented Generation (RAG, introduced in 2017). RAG combines elements of retrieval-based and generation-based approaches to generate text. It involves retrieving relevant information from a knowledge base or corpus and incorporating it into the generated text. The above figure gives us a high level overview of how RAG works, courtesy of Jaos Pambou A newer architecture, called GritLM, was developed by Contextual AI in 2024 to address the limitations of RAG. Namely, traditional RAG is computationally far from optimal, because the query and the documents have to be encoded twice: once by the embedding model for retrieval, and once by the language model as context. (GritLM , 2024) This model is a combination of a generative and retrieval based model, and improves the downstream performance and querying time. ",
  "wordCount" : "1449",
  "inLanguage": "en",
  "datePublished": "0001-01-01T00:00:00Z",
  "dateModified": "0001-01-01T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Ali Hindy"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "http://localhost:1313/blog/nlp-from-contextual-ai/natural-language-processing-99c197b09a1f451aa77fb5be55663fb7/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Ali Hindy",
    "logo": {
      "@type": "ImageObject",
      "url": "http://localhost:1313/favicon.ico"
    }
  }
}
</script>



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css" integrity="sha384-wcIxkf4k558AjM3Yz3BBFQUbk/zgIYC2R0QpeeYb+TwlBVMrlgLqwRjRtGZiK7ww" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.js" integrity="sha384-hIoBPJpTUs74ddyc4bFZSM1TVlQDA60VBbJS0oA934VSz82sBx1X7kSx2ATBDIyd" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/contrib/auto-render.min.js" integrity="sha384-43gviWU0YVjaDtb/GhzOouOXtZMP/7XUzwPTstBeZFe/+rCMvRwr4yROQP43s0Xk" crossorigin="anonymous"
  onload="renderMathInElement(document.body);"></script>

<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
          delimiters: [
            {left: '$$', right: '$$', display: true},
            {left: '$', right: '$', display: false},
            {left: "\\begin{equation}", right: "\\end{equation}", display: true},
            {left: "\\begin{equation*}", right: "\\end{equation*}", display: true},
            {left: "\\begin{align}", right: "\\end{align}", display: true},
            {left: "\\begin{align*}", right: "\\end{align*}", display: true},
            {left: "\\begin{alignat}", right: "\\end{alignat}", display: true},
            {left: "\\begin{gather}", right: "\\end{gather}", display: true},
            {left: "\\begin{CD}", right: "\\end{CD}", display: true},
          ],
          throwOnError : false
        });
    });
</script>
 


</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="Ali Hindy">
                <img src="http://localhost:1313/favicon.ico" alt="" aria-label="logo"
                    height="18"
                    width="18">Ali Hindy</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/papers/" title="Photography">
                    <span>Photography</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/courses/" title="Projects">
                    <span>Projects</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/blog/" title="Blog">
                    <span>Blog</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/books/" title="Books">
                    <span>Books</span>
                </a>
            </li>
        </ul>
    </nav>
</header>

    <main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      
    </h1>
    <div class="post-meta">7 min&nbsp;&middot;&nbsp;Ali Hindy

</div>
  </header> 
  <div class="post-content"><h1 id="natural-language-processing">Natural Language Processing</h1>
<p><strong>What is Natural Language Processing (NLP)?</strong></p>
<ul>
<li>NLP is a field of computer science focused on dealing with words and human language, and building models to do operations on our language, from machine translation to text summarization to question answering. NLP has gained significant traction in the past three years with the advent of GPT-3.5 (ChatGPT), and will only get more and more relevant as time goes on and people build better and better models.</li>
<li>At the core of NLP there exists two fundamental tasks - representation and generation, which allow us to build on to more complex ideas such as understanding or reasoning (specifically - sentiment analysis, question answering). Representation is the task of taking human tokens (such as natural English) and representing each token as a vector in high dimensional space. One desirata for these vectors is that they encode semantic meaning, and we can generate embeddings (vectors) for essentially anything, from words to sentences to entire passages.</li>
<li>We have three phases of training our models - pretraining, finetuning, and inference. When a model is pre-trained, it means that it has been trained on a vast amount of text data using a technique called self-supervised learning. During pre-training, the model learns to predict missing words in a sentence, mask certain tokens, or predict the next token in a sequence, without needing explicit labels for each task. This process allows the model to develop a rich understanding of the language and its nuances.</li>
<li>Finetuning adapts the pre-trained model to a specific dataset using supervised learning. Essentially take the pre-trained model and throw it more data in a specific use case to improve its contextualized performance. As one can imagine, one of the largest issues here is quality data curation, as your model is only as good as the data you give it.</li>
<li>Now, let’s go over the two main model architectures in current NLP (as of Feb 2024) for processing sequential data - Recurrent Neural Networks (RNNs) and Transformers. The fundamental building blocks of these models are called encoders and decoders. An encoder takes some raw input data (say, raw text) and outputs a probability density function, specifying which word is most likely to come next in a sentence. The layer before this output is called the embedding layer which encodes our words into vectors with semantic meaning. A decoder takes those embedding layers as input and returns a sequence of tokens as output.</li>
<li>A model with an encoder-decoder architecture is called a seq2seq (sequence to sequence) model, which takes in a sequence of tokens and returns a sequence of tokens. RNNs follow this type of architecture, with the most common architecture called a Long Short Term Memory (LSTM) model used for sentiment analysis or machine translation. LSTMs are designed to handle short-term and long-term dependencies in sequential data by maintaining a memory cell that can retain information over multiple time steps. However, they may struggle to capture very long-range dependencies due to issues like vanishing gradients.</li>
</ul>
<p><img loading="lazy" src="Natural%20Language%20Processing%2099c197b09a1f451aa77fb5be55663fb7/Untitled.png" alt="Untitled"  />
</p>
<ul>
<li>A seminal research paper called “Attention is All You Need” came out in 2017 and revolutionized the field of NLP. This paper introduced the notion of attention, which allows the model to focus on specific parts of the input sequence when making predictions or generating outputs, enabling it to capture long-range dependencies and improve performance on various NLP tasks. Attention operates using a key, value, query mechanism, which allows the model to compute attention scores, essentially telling the model what part of the sequence to pay more attention to.
<ul>
<li>
<p>In the traditional encoder-decoder schema of a seq2seq model, our encoder and decoders are both RNNs that keep a hidden state after each time step $t$. At the last time step of the encoder model, we pass the hidden state $h_{t_n}$ as input to the decoder as context for the decoder. $h_{t_n}$ represents our representation of the input sequence. With attention, we pass as input to the decoder $h_{t_1},&hellip;,h_{t_n}$, all of the hidden layers from each time step of the encoder. Now, at every time step of the decoder, we assign a score to the hidden states, normalize these scores with softmax, and multiply each vector by its score to give more priority to higher score values. Then we take the sum of the weighted vectors to produce our output.</p>
<p><img loading="lazy" src="Natural%20Language%20Processing%2099c197b09a1f451aa77fb5be55663fb7/Untitled%201.png" alt="Untitled"  />
</p>
</li>
<li>
<p>Example of how attention works, as we have higher values at each time step of the decoder to reflect paying more attention to different parts of the sentence. More information about attention can be found <a href="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/" target="_blank">here</a>
.</p>
</li>
</ul>
</li>
<li>The notion of attention led to the transformer architecture. Transformers process the entire input sequence in parallel, allowing them to capture long-range dependencies more efficiently. Transformers use self-attention mechanisms to weigh the importance of different tokens in the input sequence, and most importantly do not have a fixed size memory cell like an LSTM. The most popular models nowadays (GPT, LLama, etc.) are some form of a transformer model.</li>
</ul>
<p><img loading="lazy" src="Natural%20Language%20Processing%2099c197b09a1f451aa77fb5be55663fb7/Untitled%202.png" alt="Untitled"  />
</p>
<p><img loading="lazy" src="Natural%20Language%20Processing%2099c197b09a1f451aa77fb5be55663fb7/Untitled%203.png" alt="Untitled"  />
</p>
<ul>
<li>We can take a look a bit under the hood of how transformers actually work:
<ul>
<li>In the figures above, we have $N=6$ encoders stacked on top of each other (note they do not share weights) and $N$ decoders stacked on top of each other. Each encoder consists of a self-attention (or multi-head attention, just self-attention done $x$ times), layer that feeds to a feed-forward neural network. The decoder is the same thing, except it receives attention values from the encoders, just like how we described attention before.
<ul>
<li>
<p>Self-attention works very similarly to attention, except it is a way of looking through at other positions in our input sentence to provide more context for each word. For example, in the sentence “<code>The animal didn't cross the street because it was too tired</code>&quot;, self-attention allows us to determine what “it” refers to in the context of the sentence.</p>
</li>
<li>
<p>In calculating self-attention, create a query, key, and value matrix for each word. Our score for each word w.r.t word$_i$ is the dot product of the query vector for word$_i$ and the key vector of the word we are scoring. Then softmax all of these values (to normalize between 0 and 1), multiply each value vector by these scores and we get our result.</p>
<p><img loading="lazy" src="Natural%20Language%20Processing%2099c197b09a1f451aa77fb5be55663fb7/Untitled%204.png" alt="Untitled"  />
</p>
</li>
<li>
<p>This is what everything looks like when we add multi-head self-attention, and normalize after each step. The decoder looks the exact same, except it receives mutli-head attention from the last encoder (called $K$ and $V$).</p>
<p><img loading="lazy" src="Natural%20Language%20Processing%2099c197b09a1f451aa77fb5be55663fb7/Untitled%205.png" alt="Untitled"  />
</p>
</li>
<li>
<p>Wait a second? How in the world do we go from the output of a decoder, which is a hidden state vector, to a word? We add one final linear layer and a softmax layer, which converts the hidden state vector to a much larger, sparse logits vector representing a one-hot encoding of our dictionary. After a softmax, we get a probability distribution over our dictionary, and we choose the word with the highest probability.</p>
<ul>
<li>
<p>Our label distribution might look like:</p>
<p><img loading="lazy" src="Natural%20Language%20Processing%2099c197b09a1f451aa77fb5be55663fb7/Untitled%206.png" alt="Untitled"  />
</p>
</li>
<li>
<p>Whereas our training output might look like:</p>
<p><img loading="lazy" src="Natural%20Language%20Processing%2099c197b09a1f451aa77fb5be55663fb7/Untitled%207.png" alt="Untitled"  />
</p>
</li>
<li>
<p>Where at each time step, <em>we are essentially comparing two probability distributions and minimizing the difference between the two on average throughout our entire dataset.</em></p>
</li>
</ul>
</li>
</ul>
</li>
<li>To put the transformer architecture in our framework of representation and generation, we can think of the stack of encoders as learning hierarchical, expressive representations (as opposed to just having one encoder). By having multiple encoders, we are able to learn different aspects of the input sequence’s representations to better understand complex information. Furthermore, we can think of the stack of decoders as generating a more complex, expressive sequence given our hierarchical, complex understanding of the input sequence from the encoders.</li>
</ul>
</li>
<li>Now let’s move to the next phase of an NLP pipeline - finetuning. In fine-tuning, the pre-trained model is typically initialized with weights learned from a large corpus or dataset using techniques like self-supervised learning. It is then fine-tuned on a smaller, task-specific dataset using supervised learning techniques.</li>
<li>One interesting development that has taken off recently is Retrieval Augmented Generation (RAG, introduced in 2017). RAG combines elements of retrieval-based and generation-based approaches to generate text. It involves retrieving relevant information from a knowledge base or corpus and incorporating it into the generated text.</li>
</ul>
<p><img loading="lazy" src="Natural%20Language%20Processing%2099c197b09a1f451aa77fb5be55663fb7/Untitled%208.png" alt="Untitled"  />
</p>
<p><img loading="lazy" src="Natural%20Language%20Processing%2099c197b09a1f451aa77fb5be55663fb7/Untitled%209.png" alt="Untitled"  />
</p>
<ul>
<li>The above figure gives us a high level overview of how RAG works, courtesy of Jaos <a href="https://www.smashingmagazine.com/2024/01/guide-retrieval-augmented-generation-language-models/" target="_blank">Pambou</a>
</li>
<li>A newer architecture, called GritLM, was developed by Contextual AI in 2024 to address the limitations of RAG. Namely, traditional <em><strong>RAG is computationally far from optimal, because the query and the documents have to be encoded</strong></em> <em><strong>twice:</strong></em> once by the embedding model for retrieval, and once by the language model as context. (<a href="https://contextual.ai/training-with-grit/" target="_blank">GritLM</a>
, 2024) This model is a combination of a generative and retrieval based model, and improves the downstream performance and querying time.</li>
</ul>

  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
<nav class="paginav">
  <a class="prev" href="http://localhost:1313/blog/how-money-works-56ec3025eb0242db826b1404d372fa16/">
    <span> &larr;&nbsp;</span>
  </a>
</nav>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share  on x"
            href="https://x.com/intent/tweet/?text=&amp;url=http%3a%2f%2flocalhost%3a1313%2fblog%2fnlp-from-contextual-ai%2fnatural-language-processing-99c197b09a1f451aa77fb5be55663fb7%2f&amp;hashtags=">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share  on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=http%3a%2f%2flocalhost%3a1313%2fblog%2fnlp-from-contextual-ai%2fnatural-language-processing-99c197b09a1f451aa77fb5be55663fb7%2f&amp;title=&amp;summary=&amp;source=http%3a%2f%2flocalhost%3a1313%2fblog%2fnlp-from-contextual-ai%2fnatural-language-processing-99c197b09a1f451aa77fb5be55663fb7%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share  on reddit"
            href="https://reddit.com/submit?url=http%3a%2f%2flocalhost%3a1313%2fblog%2fnlp-from-contextual-ai%2fnatural-language-processing-99c197b09a1f451aa77fb5be55663fb7%2f&title=">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share  on facebook"
            href="https://facebook.com/sharer/sharer.php?u=http%3a%2f%2flocalhost%3a1313%2fblog%2fnlp-from-contextual-ai%2fnatural-language-processing-99c197b09a1f451aa77fb5be55663fb7%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share  on whatsapp"
            href="https://api.whatsapp.com/send?text=%20-%20http%3a%2f%2flocalhost%3a1313%2fblog%2fnlp-from-contextual-ai%2fnatural-language-processing-99c197b09a1f451aa77fb5be55663fb7%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share  on telegram"
            href="https://telegram.me/share/url?text=&amp;url=http%3a%2f%2flocalhost%3a1313%2fblog%2fnlp-from-contextual-ai%2fnatural-language-processing-99c197b09a1f451aa77fb5be55663fb7%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share  on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=&u=http%3a%2f%2flocalhost%3a1313%2fblog%2fnlp-from-contextual-ai%2fnatural-language-processing-99c197b09a1f451aa77fb5be55663fb7%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2024 <a href="http://localhost:1313/">Ali Hindy</a></span> ·     
    <span>
    Powered by 
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/pmichaillat/hugo-website/" rel="noopener" target="_blank">a modified version</a>
         of 
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>
</html>
